{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_kev.ipynb","provenance":[],"collapsed_sections":["vXvgjqK6AmDi"],"authorship_tag":"ABX9TyNQa3GlxJrUIhPX013Yvvhv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"s9FqwS9N6uqo"},"source":["from google import colab\n","colab.drive.mount('/content/gdrive')\n","from collections import defaultdict\n","\n","import numpy as np\n","import torch\n","from glob import glob\n","\n","from tqdm import tqdm \n","from torch.utils.data import DataLoader,TensorDataset\n","import torch.optim as optim\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","from torchvision import io\n","from torchvision import datasets, transforms, models\n","from torchsummary import summary\n","import torchvision.models as models\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import gc\n","import shutil\n","import tarfile\n","import os\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":231},"id":"pbgnYG909HGM","executionInfo":{"status":"error","timestamp":1617557785545,"user_tz":-120,"elapsed":1032,"user":{"displayName":"William","photoUrl":"","userId":"01309109182281160360"}},"outputId":"c9946e33-23d5-4716-f11e-3a3c894edcb0"},"source":["data_path = '/content/gdrive/MyDrive/IDAO'\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","params = {\n","    'batch_size': 32,\n","    'shuffle': True,\n","    'num_workers':2\n","    }\n","lr = 0.0001"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-a5086aa888e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/gdrive/MyDrive/IDAO'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m params = {\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"htQkvoN69Unj"},"source":["# Load data"]},{"cell_type":"code","metadata":{"id":"XBs2ymEJ9O7G"},"source":["%%time\n","\n","# Transfer data to the machine\n","shutil.copyfile(f'{data_path}/raw_data/track_1.tar', 'track_1.tar') \n","\n","my_tar = tarfile.open('track_1.tar')\n","my_tar.extractall('extract') # specify which folder to extract to\n","my_tar.close()\n","\n","if not os.path.exists('/content/train'):\n","    os.mkdir('/content/train')\n","\n","for img_ER in glob(\"/content/extract/idao_dataset/train/ER/*.png\"):\n","  nrj = img_ER.split(\"_\")[7]\n","  path = f'/content/train/{nrj}ER'\n","  if not os.path.exists(path):\n","    os.mkdir(path)\n","  shutil.move(img_ER, f\"{path}/{img_ER.split('/')[-1]}\")\n","\n","for img_NR in glob(\"/content/extract/idao_dataset/train/NR/*.png\"):\n","  nrj = img_NR.split(\"_\")[8]\n","  path = f'/content/train/{nrj}NR'\n","  if not os.path.exists(path):\n","    os.mkdir(path)\n","  shutil.move(img_NR, f\"{path}/{img_NR.split('/')[-1]}\")\n","\n","os.remove('track_1.tar')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"THSyTgQ09bbj"},"source":["## Splits creation"]},{"cell_type":"code","metadata":{"id":"Mu6fVIk79aQT"},"source":["train_splits1 = [[\"6NR\", \"10ER\", \"20NR\", \"30ER\"], [\"1NR\", \"3ER\", \"20NR\", \"30ER\"], [\"1NR\", \"3ER\", \"6NR\", \"10ER\"]]\n","val_splits1 = [[\"1NR\", \"3ER\"], [\"6NR\", \"10ER\"], [\"20NR\", \"30ER\"]]\n","\n","dict_splits1 = defaultdict(dict)\n","for i, (train, val) in enumerate(zip(train_splits1, val_splits1)):\n","  dict_splits1[i][\"train\"] = [glob(f\"/content/train/{group}/*.png\") for group in train]\n","  dict_splits1[i][\"train\"] = [item for sublist in dict_splits1[i][\"train\"] for item in sublist]\n","  dict_splits1[i][\"val\"] = [glob(f\"/content/train/{group}/*.png\") for group in val]\n","  dict_splits1[i][\"val\"] = [item for sublist in dict_splits1[i][\"val\"] for item in sublist]\n","\n","train_splits2 = [[\"6NR\", \"3ER\", \"20NR\", \"30ER\"], [\"1NR\", \"10ER\", \"20NR\", \"30ER\"], [\"1NR\", \"3ER\", \"6NR\", \"10ER\"]]\n","val_splits2 = [[\"1NR\", \"10ER\"], [\"6NR\", \"3ER\"], [\"20NR\", \"30ER\"]]\n","\n","dict_splits2 = defaultdict(dict)\n","for i, (train, val) in enumerate(zip(train_splits2, val_splits2)):\n","  dict_splits2[i][\"train\"] = [glob(f\"/content/train/{group}/*.png\") for group in train]\n","  dict_splits2[i][\"train\"] = [item for sublist in dict_splits2[i][\"train\"] for item in sublist]\n","  dict_splits2[i][\"val\"] = [glob(f\"/content/train/{group}/*.png\") for group in val]\n","  dict_splits2[i][\"val\"] = [item for sublist in dict_splits2[i][\"val\"] for item in sublist]\n","\n","train_splits3 = [[\"6NR\", \"3ER\", \"20NR\", \"10ER\"], [\"1NR\", \"30ER\", \"20NR\", \"10ER\"], [\"1NR\", \"30ER\", \"6NR\", \"3ER\"]]\n","val_splits3 = [[\"1NR\", \"30ER\"], [\"6NR\", \"3ER\"], [\"20NR\", \"10ER\"]]\n","\n","dict_splits3 = defaultdict(dict)\n","for i, (train, val) in enumerate(zip(train_splits3, val_splits3)):\n","  dict_splits3[i][\"train\"] = [glob(f\"/content/train/{group}/*.png\") for group in train]\n","  dict_splits3[i][\"train\"] = [item for sublist in dict_splits3[i][\"train\"] for item in sublist]\n","  dict_splits3[i][\"val\"] = [glob(f\"/content/train/{group}/*.png\") for group in val]\n","  dict_splits3[i][\"val\"] = [item for sublist in dict_splits3[i][\"val\"] for item in sublist]\n","\n","\n","train_splits4 = [[\"6NR\", \"3ER\", \"20NR\", \"10ER\"], [\"1NR\", \"3ER\", \"20NR\", \"10ER\"], [\"1NR\", \"10ER\", \"6NR\", \"3ER\"]]\n","val_splits4 = [[\"1NR\", \"30ER\"], [\"6NR\", \"30ER\"], [\"20NR\", \"30ER\"]]\n","\n","dict_splits4 = defaultdict(dict)\n","for i, (train, val) in enumerate(zip(train_splits4, val_splits4)):\n","  dict_splits4[i][\"train\"] = [glob(f\"/content/train/{group}/*.png\") for group in train]\n","  dict_splits4[i][\"train\"] = [item for sublist in dict_splits4[i][\"train\"] for item in sublist]\n","  dict_splits4[i][\"val\"] = [glob(f\"/content/train/{group}/*.png\") for group in val]\n","  dict_splits4[i][\"val\"] = [item for sublist in dict_splits4[i][\"val\"] for item in sublist]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KKBi7rd9-40s"},"source":["# Torch Dataset"]},{"cell_type":"code","metadata":{"id":"Uv46NPGo-6kB"},"source":["class Dataset(torch.utils.data.Dataset):\n","  'Characterizes a dataset for PyTorch'\n","  def __init__(self, paths, train, rot=False, flip=False, gblur=-1):\n","    'Initialization'\n","    self.paths = paths\n","    self.train = train\n","    self.rot = rot\n","    self.flip = flip \n","    self.gblur = gblur\n","\n","  def __len__(self):\n","    'Denotes the total number of samples'\n","    return len(self.paths)\n","\n","  def __getitem__(self, index):\n","    'Generates one sample of data'\n","    # Select sample\n","    ID = self.paths[index]\n","    # Load data and get label\n","    X = io.read_image(ID)\n","    X = transforms.ConvertImageDtype(torch.float32).forward(X)\n","    \n","    if self.train :\n","      if self.rot:\n","        X = transforms.RandomRotation(180).forward(X)\n","      X = transforms.CenterCrop(256).forward(X)\n","      if self.flip:\n","        X = transforms.RandomHorizontalFlip(p=.5).forward(X)\n","        X = transforms.RandomVerticalFlip(p=.5).forward(X)\n","      if self.gblur>0:\n","        X = transforms.GaussianBlur(3, sigma=(0.01, self.gblur)).forward(X)\n","    else :\n","      X = transforms.CenterCrop(256).forward(X)\n","\n","    classif_label = 1 if ID.split(\"/\")[3][-2:] == \"ER\" else 0\n","    regression_label = float(ID.split(\"/\")[3][:-2])\n","    \n","    y = (classif_label, regression_label)\n","\n","    return X, y"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KkxZwjEdAjdj"},"source":["# Train utils"]},{"cell_type":"markdown","metadata":{"id":"vXvgjqK6AmDi"},"source":["### Fit / Validation"]},{"cell_type":"code","metadata":{"id":"cUxVlE59AlNL"},"source":["def fit_kev(model, criterion_kev, data_loader, device, optimizer):\n","  running_kev_loss = 0.\n","  model.train()\n","\n","  for X, (_, y_kev) in tqdm(data_loader, total=int(len(data_loader.dataset)/data_loader.batch_size), position=0):\n","    X = X.to(device)\n","    y_kev =  y_kev.reshape(-1, 1).type(torch.float).to(device)\n","    \n","    optimizer.zero_grad()\n","    pred_kev = model(X)\n","\n","    kev_loss = criterion_kev(pred_kev, y_kev)\n","\n","    running_kev_loss += kev_loss\n","    loss.backward()\n","    optimizer.step()\n","\n","  train_kev_loss = running_kev_loss / len(data_loader.dataset)\n","  return train_kev_loss\n","\n","\n","\n","def fit_kev_l2(model, criterion_kev, data_loader, device, optimizer):\n","  running_loss = 0.\n","  running_kev_loss = 0.\n","  model.train()\n","\n","  for X, (_, y_kev) in tqdm(data_loader, total=int(len(data_loader.dataset)/data_loader.batch_size), position=0):\n","    X = X.to(device)\n","    y_kev =  y_kev.reshape(-1, 1).type(torch.float).to(device)\n","    \n","    optimizer.zero_grad()\n","    pred_kev = model(X)\n","    \n","    l2_reg = Variable(torch.FloatTensor(1), requires_grad=True)\n","    l2_reg = l2_reg.to(device)\n","    for W in model.parameters():\n","        l2_reg = l2_reg + W.norm(2)\n","\n","    kev_loss = criterion_kev(pred_kev, y_kev)\n","\n","    loss =   kev_loss + 0.01*l2_reg\n","    running_loss += loss.item()\n","    running_kev_loss += kev_loss\n","    loss.backward()\n","    optimizer.step()\n","\n","  train_loss = running_loss / len(data_loader.dataset)\n","  train_kev_loss = running_kev_loss / len(data_loader.dataset)\n","  return train_kev_loss\n","\n","def validate_kev(model, criterion_kev, data_loader, device):\n","  running_loss = 0.\n","  running_kev_loss = 0.\n","  model.eval()\n","\n","  list_kev_true = []\n","  list_kev_pred = []\n","\n","  with torch.no_grad():\n","    for X, (_,y_kev ) in data_loader:\n","      X = X.to(device)\n","      y_kev =  y_kev.reshape(-1, 1).type(torch.float).to(device)\n","\n","      pred_kev = model(X)\n","      loss = criterion_kev(pred_kev, y_kev)\n","\n","      list_kev_true += y_kev.cpu().detach().numpy().ravel().tolist()\n","      list_kev_pred += pred_kev.cpu().detach().numpy().ravel().tolist()\n","\n","      running_loss += loss.item()\n","\n","  test_loss = running_loss / len(data_loader.dataset)\n","  test_kev_loss = running_kev_loss / len(data_loader.dataset)\n","\n","  res_df = pd.DataFrame(\n","      {\n","      'kev_true': list_kev_true,\n","      'kev_pred': list_kev_pred\n","       })\n","  \n","  return test_loss, res_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mNDRhkqHB4mC"},"source":["### Models"]},{"cell_type":"code","metadata":{"id":"Uvh0grnzB6NT"},"source":["def make_resnet18():\n","  model = models.resnet18(pretrained=False)\n","  model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  model.fc = nn.Sequential(\n","      nn.Linear(in_features=512, out_features=128, bias=True),\n","      nn.Dropout(p=0.2),\n","      nn.ReLU(),\n","      nn.Linear(in_features=128, out_features=1))\n","  if torch.cuda.is_available():\n","      model.to(device)\n","  return model\n","\n","\n","def make_mobilenet_v2():\n","  model = models.mobilenet_v2(pretrained=False)\n","  model.features[0] = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","  model.classifier = nn.Sequential(\n","          nn.Linear(in_features=1280, out_features=128, bias=True),\n","          nn.Dropout(p=0.5),\n","          nn.ReLU(),\n","          nn.Linear(in_features=128, out_features=1))\n","  if torch.cuda.is_available():\n","      model.to(device)\n","  return model\n","\n","\n","def make_squeezenet1_0():\n","  model = models.squeezenet1_0(pretrained=False)\n","  model.features[0] = nn.Conv2d(1, 96, kernel_size=(7, 7), stride=(2, 2), bias=False)\n","  model.classifier = nn.Sequential(\n","        nn.Dropout(p=0.2),\n","        nn.Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1)),\n","        nn.ReLU(),\n","        nn.Flatten(),\n","        nn.Dropout(p=0.2),\n","        nn.Linear(in_features=64*15*15,out_features=512),\n","        nn.Dropout(p=0.5),\n","        nn.Linear(in_features=512,out_features=1))\n","  if torch.cuda.is_available():\n","      model.to(device)\n","  return model\n","\n","\n","def make_mobilenet_v3_small():\n","  model = models.mobilenet_v3_small(pretrained=False)\n","  model.features[0] = nn.Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","  model.classifier = nn.Sequential(\n","      nn.Linear(in_features=576, out_features=128, bias=True),\n","      nn.Hardswish(),\n","      nn.Dropout(p=0.2),\n","      nn.Linear(in_features=128, out_features=1))\n","  if torch.cuda.is_available():\n","        model.to(device)\n","  return model\n","\n","\n","def make_resnext50_32x4d():\n","  model = models.resnext50_32x4d(pretrained=False)\n","  model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  model.fc = nn.Sequential(\n","      nn.Linear(in_features=2048, out_features=128, bias=True),\n","      nn.Dropout(p=0.2),\n","      nn.ReLU(),\n","      nn.Linear(in_features=128, out_features=1))\n","  if torch.cuda.is_available():\n","    model.to(device)\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GNpj4hnICbFz"},"source":["### Functions"]},{"cell_type":"code","metadata":{"id":"3lXAdxPDCcgT"},"source":["def pred_kev(model,data_loader):\n","  model.eval()\n","  list_class_true = []\n","  list_kev_pred = []\n","  list_kev_true = []\n","  with torch.no_grad():\n","    for X, (y_class, y_kev) in tqdm(data_loader,position=0):\n","      X = X.to(device)\n","      pred_kev = model(X)\n","      list_kev_true += y_kev.cpu().detach().numpy().ravel().tolist()\n","      list_kev_pred += pred_kev.cpu().detach().numpy().ravel().tolist()\n","      list_class_true += y_class.cpu().detach().numpy().ravel().tolist()\n","  res_df = pd.DataFrame(\n","      {\n","      'class_true': list_class_true,\n","      'kev_true' : list_kev_true,\n","      'kev_pred': list_kev_pred\n","       })\n","  return res_df\n","\n","\n","def train_model(model,train_loader,test_loader,fit,optimizer,criterion,path):\n","  nb_epochs = 25\n","\n","  train_loss = []\n","  test_loss = []\n","\n","  best_loss = 10**4\n","  nb_stag = 1\n","  for i in range(nb_epochs):\n","    tmp_train_loss = fit(\n","        model=model,\n","        criterion_kev=criterion,\n","        data_loader=train_loader,\n","        device='cuda',\n","        optimizer=optimizer\n","        )\n","\n","    tmp_test_loss, _ = validate_kev(\n","        model=model,\n","        criterion_kev=criterion,\n","        data_loader=test_loader,\n","        device='cuda'\n","        )\n","\n","    if tmp_test_loss < best_loss :\n","      nb_stag = 1\n","      best_loss = tmp_test_loss\n","      torch.save(model,f'{path}/best_model.pth')\n","      print(f'\\nEpoch {i}/{nb_epochs}')\n","      print(f'Train : classif : {tmp_train_loss:.6f}')\n","      print(f'Test  : classif : {tmp_test_loss:.6f}')\n","    else :\n","      nb_stag += 1\n","    train_loss += [tmp_train_loss]\n","    test_loss += [tmp_test_loss]\n","\n","    if nb_stag>=10 : \n","      break\n","    \n","  model = torch.load(f'{path}/best_model.pth')\n","  df_res = pred_kev(model, test_loader)\n","  df_res.to_csv(f'{path}/df_res.csv', index=False)\n","\n","  plt.figure(figsize=(15,10))\n","  plt.title('loss')\n","  plt.plot(train_loss,label='train')\n","  plt.plot(test_loss,label='test')\n","  plt.legend()\n","  plt.savefig(f'{path}/loss.png')\n","\n","\n","def cross_val_kev(model_maker,dict_splits,fit,criterion,path,rot,flip,gblur):\n","  if not os.path.exists(path):\n","    os.mkdir(path)\n","  for n_split in range(len(dict_splits)):\n","    print(f'\\nCV split {n_split+1}/{len(dict_splits)}\\n')\n","\n","    training_set = Dataset(dict_splits[n_split][\"train\"], train=True, rot=rot, flip=flip, gblur=gblur)\n","    training_generator = torch.utils.data.DataLoader(training_set, **params)\n","    test_set = Dataset(dict_splits[n_split][\"val\"], train=False)\n","    test_generator = torch.utils.data.DataLoader(test_set, **params)\n","\n","    model = model_maker()\n","    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n","    cv_path = f'{path}/cv_{n_split}'\n","    if not os.path.exists(cv_path):\n","      os.mkdir(cv_path)\n","    train_model(\n","        model=model,\n","        train_loader=training_generator,\n","        test_loader=test_generator,\n","        fit=fit,\n","        optimizer=optimizer,\n","        criterion=criterion,\n","        path=cv_path)\n","  recap_kev_pred(path,True)\n","\n","\n","def recap_kev_pred(path, savefig=True):\n","  df_res = pd.DataFrame()\n","  for i in range(3):\n","    tmp_res = pd.read_csv(f'{path}/cv_{i}/df_res.csv')\n","    df_res = pd.concat([df_res,tmp_res])\n","  df_res.reset_index(drop=True,inplace=True)\n","\n","  plt.figure(figsize=(10,7))\n","  for kev in sorted(df_res.kev_true.unique()):\n","    tmp_df = df_res.loc[df_res.kev_true==kev]\n","    plt.plot(tmp_df.kev_pred.values,'.',label=kev)\n","  plt.legend()\n","  mae = mean_absolute_error(df_res.kev_true.values, df_res.kev_pred.values)\n","  if savefig:\n","    plt.savefig(f'{path}/recap_kev_{mae:.4f}.png')\n","  else :\n","    plt.show()\n","  ### kmeans part \n","  df_res.sort_values(by=['class_true'],inplace=True)\n","  tmp_0 = df_res.loc[df_res.class_true==0].copy()\n","  tmp_1 = df_res.loc[df_res.class_true==1].copy()\n","  kmeans_0 = KMeans(n_clusters=3, random_state=0).fit(tmp_0.kev_pred.values.reshape(-1,1))\n","  kmeans_1 = KMeans(n_clusters=3, random_state=0).fit(tmp_1.kev_pred.values.reshape(-1,1))\n","  tmp_0['cluster'] = kmeans_0.labels_\n","  tmp_1['cluster'] = kmeans_1.labels_\n","  replace_0 = cluster_to_kev(kmeans_0.cluster_centers_[:,0],0)\n","  replace_1 = cluster_to_kev(kmeans_1.cluster_centers_[:,0],1)\n","  tmp_0[\"cluster\"].replace(replace_0, inplace=True)\n","  tmp_1[\"cluster\"].replace(replace_1, inplace=True)\n","  df_res = pd.concat([tmp_0,tmp_1])\n","  mae = mean_absolute_error(df_res.kev_true.values, df_res.cluster.values)\n","  plt.figure(figsize=(15,10))\n","  for i,cluster in enumerate(sorted(df_res.cluster.unique())):\n","    plt.subplot(2,3,i+1)\n","    plt.title(f'target : {cluster}')\n","    tmp_df = df_res.loc[df_res.cluster==cluster]\n","    plt.hist(tmp_df.kev_true)\n","  if savefig:\n","    plt.savefig(f'{path}/recap_kev_kmeans_{mae:.4f}.png')\n","  else :\n","    plt.show()\n","  df_res.to_csv(f'{path}/df_res.csv', index=False)\n","\n","def cluster_to_kev(clusters,pred_class):\n","  if pred_class == 0:\n","    list_kev = [1,6,20]\n","  else :\n","    list_kev = [3,10,30]\n","\n","  clusters_rank = [sorted(clusters).index(x) for x in clusters]\n","  replace_dict = {\n","      0:list_kev[clusters_rank[0]],\n","      1:list_kev[clusters_rank[1]],\n","      2:list_kev[clusters_rank[2]]\n","      }\n","  return replace_dict"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XU3lu24dEcaN"},"source":["# Train"]},{"cell_type":"code","metadata":{"id":"RtpTxcFsEdNj"},"source":["criterion_class = nn.L1Loss()\n","\n","cross_val_kev(\n","    model_maker=make_resnet18,\n","    dict_splits=dict_splits4, \n","    fit = fit_kev_l2,\n","    criterion=criterion_class,\n","    rot = True,\n","    flip = True,\n","    gblur = 2,\n","    path='/content/gdrive/MyDrive/IDAO/models/kev/resnet_l2reg_gblur2_split4'\n","    )\n"],"execution_count":null,"outputs":[]}]}