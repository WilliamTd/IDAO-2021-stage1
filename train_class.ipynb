{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_class.ipynb","provenance":[],"authorship_tag":"ABX9TyMHzl53d+PY9DXn/YuS7h2R"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V1xgeFUw0F51","executionInfo":{"status":"ok","timestamp":1617555581643,"user_tz":-120,"elapsed":52994,"user":{"displayName":"William","photoUrl":"","userId":"01309109182281160360"}},"outputId":"a5f9df8e-1fc8-412f-a291-59678c88b966"},"source":["from google import colab\n","colab.drive.mount('/content/gdrive')\n","from collections import defaultdict\n","\n","import numpy as np\n","import torch\n","from glob import glob\n","\n","from tqdm import tqdm \n","from torch.utils.data import DataLoader,TensorDataset\n","import torch.optim as optim\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","from torchvision import io\n","from torchvision import datasets, transforms, models\n","from torchsummary import summary\n","import torchvision.models as models\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import gc\n","import shutil\n","import tarfile\n","import os\n","\n","\n","from sklearn.metrics import roc_auc_score\n","\n","try : \n","  import river\n","except :\n","  !pip install git+https://github.com/online-ml/river --upgrade\n","  import river"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n","Collecting git+https://github.com/online-ml/river\n","  Cloning https://github.com/online-ml/river to /tmp/pip-req-build-7rwa0dca\n","  Running command git clone -q https://github.com/online-ml/river /tmp/pip-req-build-7rwa0dca\n","Requirement already satisfied, skipping upgrade: numpy>=1.18.1 in /usr/local/lib/python3.7/dist-packages (from river==0.1.0) (1.19.5)\n","Requirement already satisfied, skipping upgrade: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from river==0.1.0) (1.4.1)\n","Requirement already satisfied, skipping upgrade: pandas>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from river==0.1.0) (1.1.5)\n","Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->river==0.1.0) (2018.9)\n","Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->river==0.1.0) (2.8.1)\n","Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.0.1->river==0.1.0) (1.15.0)\n","Building wheels for collected packages: river\n","  Building wheel for river (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for river: filename=river-0.1.0-cp37-cp37m-linux_x86_64.whl size=1642527 sha256=8dc1119474ed6daa0e19ab0e6153777a61ae1c6e992c71e275e6f5ddf4703af4\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-ofwtqwfm/wheels/1f/de/e2/d95d67b57b9a0639417cd656aecc8e5be88665ac5b63c2bd1b\n","Successfully built river\n","Installing collected packages: river\n","Successfully installed river-0.1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ePAlSvLU0n3C","executionInfo":{"status":"ok","timestamp":1617557775998,"user_tz":-120,"elapsed":822,"user":{"displayName":"William","photoUrl":"","userId":"01309109182281160360"}}},"source":["data_path = '/content/gdrive/MyDrive/IDAO'\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","params = {\n","    'batch_size': 32,\n","    'shuffle': True,\n","    'num_workers':2\n","    }\n","lr = 0.0001\n"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6ViD2RMf0yyb"},"source":["# Load data\n"]},{"cell_type":"code","metadata":{"id":"KejV13il0u07"},"source":["%%time\n","\n","# Transfer data to the machine\n","shutil.copyfile(f'{data_path}/raw_data/track_1.tar', 'track_1.tar') \n","\n","my_tar = tarfile.open('track_1.tar')\n","my_tar.extractall('extract') # specify which folder to extract to\n","my_tar.close()\n","\n","if not os.path.exists('/content/train'):\n","    os.mkdir('/content/train')\n","\n","for img_ER in glob(\"/content/extract/idao_dataset/train/ER/*.png\"):\n","  nrj = img_ER.split(\"_\")[7]\n","  path = f'/content/train/{nrj}ER'\n","  if not os.path.exists(path):\n","    os.mkdir(path)\n","  shutil.move(img_ER, f\"{path}/{img_ER.split('/')[-1]}\")\n","\n","for img_NR in glob(\"/content/extract/idao_dataset/train/NR/*.png\"):\n","  nrj = img_NR.split(\"_\")[8]\n","  path = f'/content/train/{nrj}NR'\n","  if not os.path.exists(path):\n","    os.mkdir(path)\n","  shutil.move(img_NR, f\"{path}/{img_NR.split('/')[-1]}\")\n","\n","os.remove('track_1.tar')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l2s7kpZB1IhV"},"source":["### Splits creations"]},{"cell_type":"code","metadata":{"id":"t-_XsDAU1Hpj"},"source":["train_splits1 = [[\"6NR\", \"10ER\", \"20NR\", \"30ER\"], [\"1NR\", \"3ER\", \"20NR\", \"30ER\"], [\"1NR\", \"3ER\", \"6NR\", \"10ER\"]]\n","val_splits1 = [[\"1NR\", \"3ER\"], [\"6NR\", \"10ER\"], [\"20NR\", \"30ER\"]]\n","\n","dict_splits1 = defaultdict(dict)\n","for i, (train, val) in enumerate(zip(train_splits1, val_splits1)):\n","  dict_splits1[i][\"train\"] = [glob(f\"/content/train/{group}/*.png\") for group in train]\n","  dict_splits1[i][\"train\"] = [item for sublist in dict_splits1[i][\"train\"] for item in sublist]\n","  dict_splits1[i][\"val\"] = [glob(f\"/content/train/{group}/*.png\") for group in val]\n","  dict_splits1[i][\"val\"] = [item for sublist in dict_splits1[i][\"val\"] for item in sublist]\n","\n","train_splits2 = [[\"6NR\", \"3ER\", \"20NR\", \"30ER\"], [\"1NR\", \"10ER\", \"20NR\", \"30ER\"], [\"1NR\", \"3ER\", \"6NR\", \"10ER\"]]\n","val_splits2 = [[\"1NR\", \"10ER\"], [\"6NR\", \"3ER\"], [\"20NR\", \"30ER\"]]\n","\n","dict_splits2 = defaultdict(dict)\n","for i, (train, val) in enumerate(zip(train_splits2, val_splits2)):\n","  dict_splits2[i][\"train\"] = [glob(f\"/content/train/{group}/*.png\") for group in train]\n","  dict_splits2[i][\"train\"] = [item for sublist in dict_splits2[i][\"train\"] for item in sublist]\n","  dict_splits2[i][\"val\"] = [glob(f\"/content/train/{group}/*.png\") for group in val]\n","  dict_splits2[i][\"val\"] = [item for sublist in dict_splits2[i][\"val\"] for item in sublist]\n","\n","train_splits3 = [[\"6NR\", \"3ER\", \"20NR\", \"10ER\"], [\"1NR\", \"30ER\", \"20NR\", \"10ER\"], [\"1NR\", \"30ER\", \"6NR\", \"3ER\"]]\n","val_splits3 = [[\"1NR\", \"30ER\"], [\"6NR\", \"3ER\"], [\"20NR\", \"10ER\"]]\n","\n","dict_splits3 = defaultdict(dict)\n","for i, (train, val) in enumerate(zip(train_splits3, val_splits3)):\n","  dict_splits3[i][\"train\"] = [glob(f\"/content/train/{group}/*.png\") for group in train]\n","  dict_splits3[i][\"train\"] = [item for sublist in dict_splits3[i][\"train\"] for item in sublist]\n","  dict_splits3[i][\"val\"] = [glob(f\"/content/train/{group}/*.png\") for group in val]\n","  dict_splits3[i][\"val\"] = [item for sublist in dict_splits3[i][\"val\"] for item in sublist]\n","\n","\n","train_splits4 = [ [\"1NR\", \"3ER\", \"20NR\", \"10ER\"], [\"1NR\", \"10ER\", \"6NR\", \"3ER\"]]\n","val_splits4 = [[\"6NR\", \"30ER\"], [\"20NR\", \"30ER\"]]\n","\n","dict_splits4 = defaultdict(dict)\n","for i, (train, val) in enumerate(zip(train_splits4, val_splits4)):\n","  dict_splits4[i][\"train\"] = [glob(f\"/content/train/{group}/*.png\") for group in train]\n","  dict_splits4[i][\"train\"] = [item for sublist in dict_splits4[i][\"train\"] for item in sublist]\n","  dict_splits4[i][\"val\"] = [glob(f\"/content/train/{group}/*.png\") for group in val]\n","  dict_splits4[i][\"val\"] = [item for sublist in dict_splits4[i][\"val\"] for item in sublist]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0WxiLysC1ev8"},"source":["# Torch Dataset"]},{"cell_type":"code","metadata":{"id":"25lGBlQC1eTO"},"source":["class Dataset(torch.utils.data.Dataset):\n","  'Characterizes a dataset for PyTorch'\n","  def __init__(self, paths, train, rot=True, flip=True, gblur=-1):\n","    'Initialization'\n","    self.paths = paths\n","    self.train = train\n","    self.rot = rot\n","    self.flip = flip \n","    self.gblur = gblur\n","\n","  def __len__(self):\n","    'Denotes the total number of samples'\n","    return len(self.paths)\n","\n","  def __getitem__(self, index):\n","    'Generates one sample of data'\n","    # Select sample\n","    ID = self.paths[index]\n","    # Load data and get label\n","    X = io.read_image(ID)\n","    X = transforms.ConvertImageDtype(torch.float32).forward(X)\n","    \n","    if self.train :\n","      #if self.perspective:\n","        #X = transforms.RandomPerspective(distortion_scale=0.1, p=0.5).forward(X)\n","      if self.rot:\n","        X = transforms.RandomRotation(180).forward(X)\n","      X = transforms.CenterCrop(256).forward(X)\n","      if self.flip:\n","        X = transforms.RandomHorizontalFlip(p=.5).forward(X)\n","        X = transforms.RandomVerticalFlip(p=.5).forward(X)\n","      if self.gblur>0:\n","        X = transforms.GaussianBlur(5, sigma=(0.1, self.gblur)).forward(X)\n","    else :\n","      X = transforms.CenterCrop(256).forward(X)\n","\n","    classif_label = 1 if ID.split(\"/\")[3][-2:] == \"ER\" else 0\n","    regression_label = float(ID.split(\"/\")[3][:-2])\n","    y = (classif_label, regression_label)\n","\n","    return X, y"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dq6jldkb15nj"},"source":["# Train utils"]},{"cell_type":"markdown","metadata":{"id":"lWzvQ4ue3GJJ"},"source":["### Fit / Validation"]},{"cell_type":"code","metadata":{"id":"oIrJ7nmw1-Z5"},"source":["def fit_bin(model, criterion_class, metric, data_loader, device, optimizer):\n","  running_loss = 0.\n","\n","  model.train()\n","  for X, (y_class, _) in tqdm(data_loader, total=int(len(data_loader.dataset)/data_loader.batch_size), position=0):\n","    X = X.to(device)\n","    y_class =  y_class.reshape(-1, 1).type(torch.float).to(device)\n","\n","    optimizer.zero_grad()\n","    pred_class = model(X)\n","\n","    loss =  criterion_class(pred_class, y_class)\n","\n","    running_loss += loss.item()\n","\n","    loss.backward()\n","    optimizer.step()\n","    for y_t, y_p in zip(y_class.cpu().detach().numpy().reshape(-1), pred_class.cpu().detach().numpy().reshape(-1)):\n","            metric = metric.update(y_t, y_p)\n","  \n","  train_loss = running_loss / len(data_loader.dataset)\n","\n","  return train_loss, metric \n","\n","\n","def fit_bin_l2reg(model, criterion_class, metric, data_loader, device, optimizer):\n","  running_loss = 0.\n","  running_class_loss = 0.\n","  model.train()\n","  for X, (y_class, _) in tqdm(data_loader, total=int(len(data_loader.dataset)/data_loader.batch_size), position=0):\n","    X = X.to(device)\n","    y_class =  y_class.reshape(-1, 1).type(torch.float).to(device)\n","\n","    optimizer.zero_grad()\n","    pred_class = model(X)\n","    \n","    l2_reg = Variable( torch.FloatTensor(1), requires_grad=True)\n","    l2_reg = l2_reg.to(device)\n","    for W in model.parameters():\n","        l2_reg = l2_reg + W.norm(2)\n","\n","    class_loss = criterion_class(pred_class, y_class)\n","    loss =   class_loss + 0.01*l2_reg\n","\n","    running_loss += loss.item()\n","    running_class_loss += class_loss\n","\n","    loss.backward()\n","    optimizer.step()\n","    for y_t, y_p in zip(y_class.cpu().detach().numpy().reshape(-1), pred_class.cpu().detach().numpy().reshape(-1)):\n","            metric = metric.update(y_t, y_p)\n","  \n","  train_loss = running_loss / len(data_loader.dataset)\n","  train_class_loss = running_class_loss / len(data_loader.dataset)\n","\n","  return train_class_loss, metric \n","\n","\n","def validate_bin(model, criterion_class, metric, data_loader, device):\n","  running_loss = 0.\n","  model.eval()\n","\n","  list_class_true = []\n","  list_class_pred = []\n","\n","  with torch.no_grad():\n","    for X, (y_class,_ ) in data_loader:\n","      X = X.to(device)\n","      y_class =  y_class.reshape(-1, 1).type(torch.float).to(device)\n","\n","      pred_class = model(X)\n","\n","      list_class_true += y_class.cpu().detach().numpy().ravel().tolist()\n","      list_class_pred += pred_class.cpu().detach().numpy().ravel().tolist()\n","\n","      loss = criterion_class(pred_class, y_class)\n","\n","      running_loss += loss.item()\n","\n","      for y_t, y_p in zip(y_class.cpu().detach().numpy().reshape(-1), pred_class.cpu().detach().numpy().reshape(-1)):\n","            metric = metric.update(y_t, y_p)\n","\n","  test_loss = running_loss / len(data_loader.dataset)\n","\n","  res_df = pd.DataFrame(\n","      {\n","      'class_true': list_class_true,\n","      'class_pred': list_class_pred\n","       })\n","  \n","  return test_loss, res_df, metric"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a_i__XKo3J_Z"},"source":["### Models generation"]},{"cell_type":"code","metadata":{"id":"43UxxVla2KCs"},"source":["def make_resnet18():\n","  model = models.resnet18(pretrained=False)\n","  model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  model.fc = nn.Sequential(\n","      nn.Linear(in_features=512, out_features=256, bias=True),\n","      nn.Dropout(p=0.5),\n","      nn.ReLU(),\n","      nn.Linear(in_features=256, out_features=1),\n","      nn.Sigmoid())\n","  if torch.cuda.is_available():\n","      model.to(device)\n","  return model\n","\n","\n","def make_mobilenet_v2():\n","  model = models.mobilenet_v2(pretrained=False)\n","  model.features[0] = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","  model.classifier = nn.Sequential(\n","          nn.Linear(in_features=1280, out_features=128, bias=True),\n","          nn.Dropout(p=0.5),\n","          nn.ReLU(),\n","          nn.Linear(in_features=128, out_features=1),\n","          nn.Sigmoid())\n","  if torch.cuda.is_available():\n","      model.to(device)\n","  return model\n","\n","\n","def make_squeezenet1_0():\n","  model = models.squeezenet1_0(pretrained=False)\n","  model.features[0] = nn.Conv2d(1, 96, kernel_size=(7, 7), stride=(2, 2), bias=False)\n","  model.classifier = nn.Sequential(\n","        nn.Dropout(p=0.5),\n","        nn.Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1)),\n","        nn.ReLU(),\n","        nn.MaxPool2d(kernel_size=(3,3),stride=(3,3)),\n","        nn.Flatten(),\n","        nn.Linear(in_features=128*5*5,out_features=1),\n","        nn.Sigmoid())\n","  if torch.cuda.is_available():\n","      model.to(device)\n","  return model\n","\n","\n","def make_mobilenet_v3_small():\n","  model = models.mobilenet_v3_small(pretrained=False)\n","  model.features[0] = nn.Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","  model.classifier = nn.Sequential(\n","      nn.Linear(in_features=576, out_features=128, bias=True),\n","      nn.Hardswish(),\n","      nn.Dropout(p=0.2),\n","      nn.Linear(in_features=128, out_features=1),\n","      nn.Sigmoid())\n","  if torch.cuda.is_available():\n","        model.to(device)\n","  return model\n","\n","\n","def make_resnext50_32x4d():\n","  model = models.resnext50_32x4d(pretrained=False)\n","  model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  model.fc = nn.Sequential(\n","      nn.Linear(in_features=2048, out_features=128, bias=True),\n","      nn.Dropout(p=0.5),\n","      nn.ReLU(),\n","      nn.Linear(in_features=128, out_features=1),\n","      nn.Sigmoid())\n","  if torch.cuda.is_available():\n","    model.to(device)\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ENlonceL3W-D"},"source":["### Cross Validation"]},{"cell_type":"code","metadata":{"id":"0Sw_w9DR3ZQL","executionInfo":{"status":"ok","timestamp":1617556434604,"user_tz":-120,"elapsed":1364,"user":{"displayName":"William","photoUrl":"","userId":"01309109182281160360"}}},"source":["def pred_class(model,data_loader):\n","  model.eval()\n","  list_class_true = []\n","  list_class_pred = []\n","  list_kev_true = []\n","  with torch.no_grad():\n","    for X, (y_class, y_kev) in tqdm(data_loader,position=0):\n","      X = X.to(device)\n","      pred_class = model(X)\n","      list_class_true += y_class.cpu().detach().numpy().ravel().tolist()\n","      list_class_pred += pred_class.cpu().detach().numpy().ravel().tolist()\n","      list_kev_true += y_kev.cpu().detach().numpy().ravel().tolist()\n","  res_df = pd.DataFrame(\n","      {\n","      'class_true': list_class_true,\n","      'keV' : list_kev_true,\n","      'class_pred': list_class_pred\n","       })\n","  return res_df\n","\n","\n","def train_model(model,train_loader,test_loader,fit,optimizer,criterion,path):\n","  nb_epochs = 25\n","\n","  train_loss = []\n","  train_auc = []\n","  test_loss = []\n","  test_auc = []\n","\n","  best_loss = 10**4\n","  nb_stag = 1\n","  for i in range(nb_epochs):\n","    tmp_train_loss, tmp_train_auc = fit(\n","        model=model,\n","        criterion_class=criterion,\n","        data_loader=train_loader,\n","        metric=river.metrics.ROCAUC(),\n","        device='cuda',\n","        optimizer=optimizer\n","        )\n","\n","    tmp_test_loss, df_res, tmp_test_auc = validate_bin(\n","        model=model,\n","        criterion_class=criterion,\n","        data_loader=test_loader,\n","        metric=river.metrics.ROCAUC(),\n","        device='cuda'\n","        )\n","\n","    if tmp_test_loss < best_loss :\n","      nb_stag = 1\n","      best_loss = tmp_test_loss\n","      torch.save(model,f'{path}/best_model.pth')\n","      print(f'\\nEpoch {i}/{nb_epochs}')\n","      print(f'Train : classif : {tmp_train_loss:.4f}; AUC : {tmp_train_auc}')\n","      print(f'Test  : classif : {tmp_test_loss:.4f}; AUC : {tmp_test_auc}')\n","    else :\n","      nb_stag += 1\n","    train_loss += [tmp_train_loss]\n","    train_auc += [tmp_train_auc.get()]\n","    test_loss += [tmp_test_loss]\n","    test_auc += [tmp_test_auc.get()]\n","\n","    if nb_stag>=10 : \n","      break\n","    \n","  model = torch.load(f'{path}/best_model.pth')\n","  df_res = pred_class(model, test_loader)\n","  df_res.to_csv(f'{path}/df_res.csv', index=False)\n","  plt.figure(figsize=(15,10))\n","  plt.subplot(1,2,1)\n","  plt.title('loss')\n","  plt.plot(train_loss,label='train')\n","  plt.plot(test_loss,label='test')\n","  plt.legend()\n","  plt.subplot(1,2,2)\n","  plt.title('auc')\n","  plt.plot(train_auc,label='train')\n","  plt.plot(test_auc,label='test')\n","  plt.savefig(f'{path}/loss.png')\n","\n","\n","def cross_val(model_maker,dict_splits,fit,criterion,path,rot,flip,gblur):\n","  if not os.path.exists(path):\n","    os.mkdir(path)\n","  for n_split in range(len(dict_splits)):\n","    print(f'\\nCV split {n_split+1}/{len(dict_splits)}\\n')\n","    training_set = Dataset(dict_splits[n_split][\"train\"], train=True, rot=rot, flip=flip, gblur=gblur)\n","    training_generator = torch.utils.data.DataLoader(training_set, **params)\n","    validation_set = Dataset(dict_splits[n_split][\"val\"], train=False)\n","    validation_generator = torch.utils.data.DataLoader(validation_set, **params)\n","    model = model_maker()\n","    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n","    cv_path = f'{path}/cv_{n_split}'\n","    if not os.path.exists(cv_path):\n","      os.mkdir(cv_path)\n","    train_model(\n","        model=model,\n","        train_loader=training_generator,\n","        test_loader=validation_generator,\n","        fit=fit,\n","        optimizer=optimizer,\n","        criterion=criterion,\n","        path=cv_path)\n","  recap_class_pred(path,True)\n","  recap_roc(path,True)\n","\n","\n","def recap_class_pred(path,savefig=True):\n","  df_res = pd.DataFrame()\n","  for i in range(3):\n","    tmp_res = pd.read_csv(f'{path}/cv_{i}/df_res.csv')\n","    df_res = pd.concat([df_res,tmp_res])\n","  df_res.reset_index(drop=True,inplace=True)\n","  df_res.sort_values(by=['class_true','keV'], inplace=True)\n","  display(df_res.head())\n","  plt.figure(figsize=(15,8))\n","  plt.subplot(1,2,1)\n","  plt.title('Class 0 / ER / keV 1,6,20')\n","  plt.plot(df_res.loc[df_res.class_true==0].class_pred,'.')\n","  plt.subplot(1,2,2)\n","  plt.title('Class 1 / NR / keV 3,10,30')\n","  plt.plot(df_res.loc[df_res.class_true==1].class_pred,'.')\n","  if savefig:\n","    plt.savefig(f'{path}/recap_class_pred.png')\n","  else :\n","    plt.show()\n","  df_res.to_csv(f'{path}/df_res.csv')\n","\n","\n","def recap_roc(path,savefig=True):\n","  df_res = pd.read_csv(f'{path}/df_res.csv')\n","  roc_score = roc_auc_score(df_res.class_true, df_res.class_pred)\n","  plt.figure(figsize=(25,10))\n","  for i,split in enumerate([[1,3],[6,10],[20,30]]):\n","    tmp_res = df_res.loc[df_res.keV.isin(split)]\n","    fpr, tpr, threshold = roc_curve(tmp_res.class_true, tmp_res.class_pred)\n","    roc_auc = auc(fpr, tpr)\n","    plt.subplot(1,3,i+1)\n","    plt.title(f'ROC split : {split}')\n","    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.4f' % roc_auc)\n","    plt.legend(loc = 'lower right')\n","    plt.plot([0, 1], [0, 1],'r--')\n","    plt.xlim([0, 1])\n","    plt.ylim([0, 1])\n","    plt.ylabel('True Positive Rate')\n","    plt.xlabel('False Positive Rate')\n","  if savefig:\n","    plt.savefig(f'{path}/recap_roc_{int(10000*roc_score)}.png')\n","  else :\n","    plt.show()"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vf97NhAO40PO"},"source":["# Train"]},{"cell_type":"code","metadata":{"id":"X__nkeKI4GkT"},"source":["criterion_class = nn.BCELoss()\n","\n","cross_val(\n","    model_maker=make_resnet18,\n","    dict_splits=dict_splits2, \n","    fit = fit_bin_l2reg,\n","    criterion=criterion_class,\n","    rot = True,\n","    flip = True,\n","    gblur = 2,\n","    path='/content/gdrive/MyDrive/IDAO/models/class/resnet_l2reg_gblur2_split2'\n","    )\n"],"execution_count":null,"outputs":[]}]}